{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 5 files created...\r"
     ]
    }
   ],
   "source": [
    "url = \"https://newyork.craigslist.org/search/sss?\"\n",
    "count = 0\n",
    "no_of_ads_to_be_fetched = 5\n",
    "flag = True\n",
    "while True:\n",
    "    response = requests.get(url) #we got to the website\n",
    "    data = response.text #we fetched the HTML code / source code of the website\n",
    "    soup = BeautifulSoup(data, \"html.parser\") #cleaned up the code (Parsing HTML of the website, aka making soup)\n",
    "    ads = soup.find_all(\"div\", {\"class\": \"result-info\"})\n",
    "    for ad in ads:\n",
    "        if count >= no_of_ads_to_be_fetched:\n",
    "            flag = False\n",
    "            break\n",
    "        title = ad.find(\"a\", {\"class\": \"result-title\"}).text\n",
    "\n",
    "        price_tag = ad.find(\"span\", {\"class\": \"result-price\"})\n",
    "\n",
    "        if (price_tag):\n",
    "            price = price_tag.text\n",
    "        else:\n",
    "            price = \"Not Listed Publicly\"\n",
    "\n",
    "        link = ad.find(\"a\", {\"class\":\"result-title\"}).get('href')\n",
    "\n",
    "        #working on each individual link found and printing the contents of its web page\n",
    "        #repeating the process that we did for the homepage:\n",
    "        ad_response = requests.get(link)\n",
    "        ad_data = ad_response.text\n",
    "        ad_soup = BeautifulSoup(ad_data, \"html.parser\")\n",
    "        if(ad_soup.find(\"section\", {\"id\": \"postingbody\"})):\n",
    "            ad_description = ad_soup.find(\"section\", {\"id\": \"postingbody\"}).text\n",
    "        else:\n",
    "            ad_description = \"Descripton not available.\"\n",
    "        #ad_description = os.linesep.join([s for s in ad_description.splitlines() if s]) #just to remove empty lines\n",
    "        #import os to use the upper line\n",
    "        \n",
    "        #all_info\n",
    "        all_info = title + \"\\n\" + price + \"\\n\" + ad_description\n",
    "        \n",
    "        #saving the ads in files\n",
    "        filename = str(count + 1) + \".txt\"\n",
    "        fout = open(\"ads\\\\\" + filename, \"w\", encoding = 'utf-8')\n",
    "        fout.write(all_info)\n",
    "        fout.close()\n",
    "        if count == 0:\n",
    "            print(count + 1, \"file\", \"created...\", end=\"\\r\", flush=True)\n",
    "        else:\n",
    "             print(count + 1, \"files\", \"created...\", end=\"\\r\", flush=True)\n",
    "        \n",
    "        if count == no_of_ads_to_be_fetched - 1:\n",
    "            print(\"All\", count + 1, \"files\", \"created...\", end=\"\\r\")\n",
    "        count += 1\n",
    "\n",
    "    if flag: #if 1000 ads have NOT been fetched\n",
    "        url_tag = soup.find(\"a\", {\"title\":\"next page\"})\n",
    "        if url_tag.get('href'): #if all \"next pages\" of the site end before we reach 1000 ads, then this will handle it.\n",
    "            url = \"https://newyork.craigslist.org\" + url_tag.get('href')\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency dictionary\n",
    "frequencies = dict()\n",
    "for i in range(0, no_of_ads_to_be_fetched):\n",
    "    filename = str(i + 1) + \".txt\"\n",
    "    try: \n",
    "        fin = open(\"ads\\\\\" + filename, encoding = 'utf-8')\n",
    "    except:\n",
    "        print(\"Failed to open\", filename)\n",
    "        \n",
    "    #read, case-normalize and tokenize:\n",
    "    words = fin.read().lower().split() #all words from file in lower case but CONTAMINATED w/punctuations\n",
    "    words = str(words).translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation))) #de-contaminated STRING\n",
    "    words = words.split() #de-contaminated LIST\n",
    "    \n",
    "    for word in words: #store words in dictionary\n",
    "        if word not in frequencies:\n",
    "            frequencies[word] = 1\n",
    "        else:\n",
    "            frequencies[word] += 1\n",
    "    fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning unique id to each word\n",
    "w2n = dict()\n",
    "n2w = dict()\n",
    "i = 0\n",
    "for k,v in frequencies.items():\n",
    "    w2n[k] = i\n",
    "    n2w[i] = k\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating document's matrix\n",
    "cols = len(frequencies)\n",
    "doc_matrix = np.zeros((no_of_ads_to_be_fetched, cols))\n",
    "\n",
    "#assigning 1 in doc matrix in the cell whose string is present in that particular file\n",
    "for i in range(0, no_of_ads_to_be_fetched):\n",
    "    for n in n2w:\n",
    "        filename = str(i + 1) + \".txt\"\n",
    "        try: \n",
    "            fin = open(\"ads\\\\\" + filename, encoding = 'utf-8')\n",
    "        except:\n",
    "            print(\"Failed to open\", filename)\n",
    "\n",
    "        reader = fin.read().lower()\n",
    "        if n2w[n] in reader:\n",
    "            doc_matrix[i, n] = 1   \n",
    "    fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraper Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to buy? Storage boxes\n"
     ]
    }
   ],
   "source": [
    "#Query handling\n",
    "query = input(\"What do you want to buy? \")\n",
    "query = query.lower().split()\n",
    "query = str(query).translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation))) #de-contaminated STRING\n",
    "query = query.split() #de-contaminated LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating query matrix\n",
    "query_matrix = np.zeros((cols))\n",
    "#Obtaining id of the queried word from w2n dictionary\n",
    "count = 0\n",
    "for token in query:\n",
    "    if token in w2n:\n",
    "        uid = w2n[token]\n",
    "        query_matrix[uid] = 1\n",
    "        count += 1\n",
    "if count == 0:\n",
    "    print(\"Your search \", query, \"did not match any documents.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dot Product\n",
    "transpose = doc_matrix.T\n",
    "dot_prod = query_matrix.dot(transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used in elimination\n",
    "descending_scores = np.sort(dot_prod)[::-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ranking the pages\n",
    "descending_filenos = np.argsort(dot_prod)[::-1][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminating files with 0 matches\n",
    "count = 0\n",
    "for score in descending_scores:\n",
    "    if score < 1:\n",
    "        break\n",
    "    else:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your results were matched in following file(s):\n",
      "3.txt\n"
     ]
    }
   ],
   "source": [
    "#Printing the matched results\n",
    "print(\"Your results were matched in following file(s):\")\n",
    "for i in range (0, count):\n",
    "    filename = str(descending_filenos[i] + 1) + \".txt\"\n",
    "    print(filename) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
